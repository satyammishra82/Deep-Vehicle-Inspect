{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KvKavindu/Car-damage-detection/blob/main/KNN_and_CNN_for_damage_detection_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HAowIwaVcNO_"
      },
      "cell_type": "markdown",
      "source": [
        "**This is the notebook that demonstrates how to classify labeled car damage images using KNN & CNN\n",
        "algorithm**\n",
        "\n",
        "This notebook loads the car damage dataset from the google drive.Then applies KNN algorithm and Custome CNN algorithm and show results\n"
      ]
    },
    {
      "metadata": {
        "id": "Kk3Nqvh9ij0e"
      },
      "cell_type": "markdown",
      "source": [
        "We will load the dataset from the google drive. For that we import google colab library ."
      ]
    },
    {
      "metadata": {
        "id": "qlIBlZ2JZKek",
        "outputId": "199d7c34-ae22-4ad8-dd5b-7484ada2b91f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XJ8R7qPSjA0a"
      },
      "cell_type": "markdown",
      "source": [
        "Lets **list** the folder in our dataset ,here the folder is car-damage"
      ]
    },
    {
      "metadata": {
        "id": "aGH5vImXvQ6Y",
        "outputId": "8ef4b3a9-4bf8-4ff1-fb32-f3e7cf12cb91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/My\\ Drive/datasets/car-damage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "damage\tundamage\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TrrpRKtIjV0b"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the class SimpleDatasetLoader which loads the dataset fom the drive and it gives data and label,which are tuple of numpy array of data and labels\n",
        "\n",
        "# Implement SimpleDatasetLoader"
      ]
    },
    {
      "metadata": {
        "id": "OH__VfWsR3ue"
      },
      "cell_type": "code",
      "source": [
        "#Class to load the dataset images from drivce\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleDatasetLoader:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, preprocessors=None):\n",
        "        \"\"\"\n",
        "        :param preprocessors: List of image preprocessors\n",
        "        \"\"\"\n",
        "        self.preprocessors = preprocessors\n",
        "\n",
        "        if self.preprocessors is None:\n",
        "            self.preprocessors = []\n",
        "\n",
        "    # Method: Used to load a list of images for pre-processing\n",
        "    def load(self, image_paths, verbose=-1):\n",
        "        \"\"\"\n",
        "        :param image_paths: List of image paths\n",
        "        :param verbose: Parameter for printing information to console\n",
        "        :return: Tuple of data and labels\n",
        "        \"\"\"\n",
        "        data, labels = [], []\n",
        "\n",
        "        for i, image_path in enumerate(image_paths):\n",
        "            image = cv2.imread(image_path)\n",
        "            label = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "            if self.preprocessors is not None:\n",
        "                for p in self.preprocessors:\n",
        "                    image = p.preprocess(image)\n",
        "\n",
        "            data.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "            if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
        "                print('[INFO]: Processed {}/{}'.format(i+1, len(image_paths)))\n",
        "\n",
        "        return (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BlK-S-PlZ9n"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Machine learning algorithm such as k-NN require all images in a dataset to have a **fixed feature vector size**.\n",
        "\n",
        "In the case of images, this\n",
        "requirement implies that our images must be preprocessed and scaled to have identical widths and heights.\n",
        "\n",
        "There are a number of ways to accomplish this resizing and scaling, ranging from more advanced methods that respect the aspect ratio of the original image to the scaled image to simple methods that ignore the aspect ratio and simply squash the width and height to the required dimensions\n",
        "\n",
        "class SimplePreprocessor builds an image preprocessor that resizes\n",
        "the image, ignoring the aspect ratio.\n"
      ]
    },
    {
      "metadata": {
        "id": "umz4t5kxmg-c"
      },
      "cell_type": "markdown",
      "source": [
        "#Implementing SimplePreprocessor"
      ]
    },
    {
      "metadata": {
        "id": "3b8mRpIbR9Tf"
      },
      "cell_type": "code",
      "source": [
        "#Class Preprocessror\n",
        "class SimplePreprocessor:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, width, height, interpolation=cv2.INTER_AREA):\n",
        "        \"\"\"\n",
        "        :param width: Image width\n",
        "        :param height: Image height\n",
        "        :param interpolation: Interpolation algorithm\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    # Method: Used to resize the image to a fixed size (ignoring the aspect ratio)\n",
        "    def preprocess(self, image):\n",
        "        \"\"\"\n",
        "        :param image: Image\n",
        "        :return: Re-sized image\n",
        "        \"\"\"\n",
        "        return cv2.resize(image, (self.width, self.height), interpolation=self.interpolation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7obCyoDtmIsd"
      },
      "cell_type": "markdown",
      "source": [
        "# Implement k-NN Classifier\n",
        "\n",
        "---\n",
        "• Step #1 –**Gather Our Dataset** : The datasets consists of 750 images with 605 and 146 damaged and undamaged images respectively. Each image is represented in the RGB color space. We will preprocess each image by resizing it to 32 × 32 pixels. Taking into account the three RGB channels, the resized image dimensions imply that each image in the dataset is represented by 32 × 32 × 3 = 3, 072 integers.\n",
        "\n",
        "• Step #2 – **Split the Dataset**: We will split the data, One split for training, and the other for testing.\n",
        "\n",
        "• Step #3 – **Train the Classifier**: Our k-NN classifier will be trained on the raw pixel intensities of the images in the training set.\n",
        "\n",
        "• Step #4 – **Evaluate**: Once our k-NN classifier is trained, we can evaluate performance on the test set."
      ]
    },
    {
      "metadata": {
        "id": "fcCvmbWGSDhk",
        "outputId": "75474112-4f61-4eb5-f216-42e304f850a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/gdrive/My Drive/datasets/car-damage\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(data, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (32, 32, 3) to (32*32*3=3072)\n",
        "data = data.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(data.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: Classification starting....')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))\n",
        "\n",
        "print('[INFO]: Classification based on kd_tree')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1,algorithm='kd_tree')\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: Images loading....\n",
            "[INFO]: Processed 500/751\n",
            "[INFO]: Features Matrix: 2253000.0MB\n",
            "[INFO]: Classification starting....\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      damage       0.77      1.00      0.87       143\n",
            "    undamage       1.00      0.04      0.09        45\n",
            "\n",
            "    accuracy                           0.77       188\n",
            "   macro avg       0.88      0.52      0.48       188\n",
            "weighted avg       0.82      0.77      0.68       188\n",
            "\n",
            "[INFO]: Classification based on kd_tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      damage       0.77      1.00      0.87       143\n",
            "    undamage       1.00      0.04      0.09        45\n",
            "\n",
            "    accuracy                           0.77       188\n",
            "   macro avg       0.88      0.52      0.48       188\n",
            "weighted avg       0.82      0.77      0.68       188\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The construction of a KD tree is very fast."
      ],
      "metadata": {
        "id": "qrPsw12b6bbx"
      }
    },
    {
      "metadata": {
        "id": "_qQVucJV3DWx"
      },
      "cell_type": "markdown",
      "source": [
        "# How to find Best K?"
      ]
    },
    {
      "metadata": {
        "id": "mbjd3lACpnSo",
        "outputId": "4011cba0-1973-4fe1-d4e8-d839a4c58c42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=2,n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "accuracy = accuracy_score(model.predict(test_x), test_y)\n",
        "print(accuracy)\n",
        "n_neighbors = np.array([7,8,9,10,12,15,20])\n",
        "param_grid = dict(n_neighbors=n_neighbors)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid.fit(train_x, train_y)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_estimator_.n_neighbors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7712765957446809\n",
            "0.8223767383059417\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IosHrvJAjzbz"
      },
      "cell_type": "markdown",
      "source": [
        "K=7 Neighbours gives best score."
      ]
    },
    {
      "metadata": {
        "id": "10Q7T9-Dsjq4"
      },
      "cell_type": "markdown",
      "source": [
        "Classifying a new testing point\n",
        "requires a comparison to every single data point in our training data, which scales O(N), making\n",
        "working with larger datasets computationally prohibitive."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "zzXWHXHxnBPb"
      }
    },
    {
      "metadata": {
        "id": "EM8AgyAC1HpA"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GB_g-yD8m_u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN = models.Sequential()\n",
        "modelCNN.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "modelCNN.add(layers.MaxPooling2D((2, 2)))\n",
        "modelCNN.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "modelCNN.add(layers.MaxPooling2D((2, 2)))\n",
        "modelCNN.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "modelCNN.add(layers.Flatten())\n",
        "modelCNN.add(layers.Dense(64, activation='relu'))\n",
        "modelCNN.add(layers.Dense(2))"
      ],
      "metadata": {
        "id": "2gX68n6-m-u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCNN.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tXAVMIEs7VH",
        "outputId": "68be8e9a-596d-419e-b217-29e5aa281a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,050\n",
            "Trainable params: 122,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/gdrive/My Drive/datasets/car-damage\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(dataC, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (3000, 32, 32, 3) to (3000, 32*32*3=3072)\n",
        "#dataC = dataC.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(dataC.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(dataC, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: CNN Classification starting....')\n",
        "\n",
        "modelCNN.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = modelCNN.fit(train_x, train_y, epochs=20,\n",
        "                    validation_data=(test_x, test_y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YmkEhS-xQ_t",
        "outputId": "f8ae2251-f26f-4cd8-85b6-46b64ebfa323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: Images loading....\n",
            "[INFO]: Processed 500/751\n",
            "[INFO]: Features Matrix: 2253000.0MB\n",
            "[INFO]: CNN Classification starting....\n",
            "Epoch 1/20\n",
            "18/18 [==============================] - 2s 63ms/step - loss: 0.4249 - accuracy: 0.8472 - val_loss: 0.7890 - val_accuracy: 0.7287\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 1s 56ms/step - loss: 0.3044 - accuracy: 0.8917 - val_loss: 0.5620 - val_accuracy: 0.7660\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.2034 - accuracy: 0.9183 - val_loss: 0.7108 - val_accuracy: 0.7553\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1937 - accuracy: 0.9396 - val_loss: 0.6919 - val_accuracy: 0.7713\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1181 - accuracy: 0.9609 - val_loss: 0.7857 - val_accuracy: 0.7500\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0899 - accuracy: 0.9734 - val_loss: 1.0664 - val_accuracy: 0.7500\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 1s 53ms/step - loss: 0.0585 - accuracy: 0.9787 - val_loss: 1.1048 - val_accuracy: 0.7819\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.0551 - accuracy: 0.9840 - val_loss: 1.0288 - val_accuracy: 0.7979\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.0908 - accuracy: 0.9645 - val_loss: 1.0917 - val_accuracy: 0.7926\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.0888 - accuracy: 0.9751 - val_loss: 1.0289 - val_accuracy: 0.7447\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.0717 - accuracy: 0.9734 - val_loss: 0.9796 - val_accuracy: 0.7606\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0365 - accuracy: 0.9929 - val_loss: 1.1358 - val_accuracy: 0.7819\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.0194 - accuracy: 0.9982 - val_loss: 1.1908 - val_accuracy: 0.7872\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0151 - accuracy: 0.9964 - val_loss: 2.3393 - val_accuracy: 0.7553\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0851 - accuracy: 0.9787 - val_loss: 1.7546 - val_accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.0934 - accuracy: 0.9716 - val_loss: 1.1630 - val_accuracy: 0.7660\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0320 - accuracy: 0.9911 - val_loss: 1.3377 - val_accuracy: 0.7500\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.5214 - val_accuracy: 0.7553\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.4924 - val_accuracy: 0.7660\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.5422 - val_accuracy: 0.7713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = modelCNN.evaluate(test_x,  test_y, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "g_pb0N6wtLDg",
        "outputId": "926e409a-df0d-4f3e-fd7f-401fa7b9ec5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 - 0s - loss: 1.5422 - accuracy: 0.7713 - 94ms/epoch - 16ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}